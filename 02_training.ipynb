{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-params scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "\n",
    "@annealer\n",
    "def sched_no(start, end, pos): return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class SlowStartCyle:\n",
    "    def __init__(self, lr_max, moms=(0.95, 0.85), div_factor=25., pct_start=0.1, final_div=None):\n",
    "        self.lr_max,self.div_factor,self.pct_start,self.final_div = lr_max,div_factor,pct_start,final_div\n",
    "        self.mom = moms\n",
    "        if final_div is None: self.final_div = div_factor*1e4\n",
    "            \n",
    "    def start_training(self, nb_iter):\n",
    "        self.first_step = sched_cos(self.lr_max/self.div_factor, self.lr_max)\n",
    "        self.last_step = sched_no(self.lr_max, self.lr_max)\n",
    "        \n",
    "        self.first_step_mom = sched_cos(self.mom[0], self.mom[1])\n",
    "        self.last_step_mom = sched_cos(self.mom[1], self.mom[0])\n",
    "        \n",
    "        self.nb_iter = nb_iter\n",
    "        \n",
    "    def get_params(self, pos):\n",
    "        current_lr = None\n",
    "        func_lr = None\n",
    "        current_pos = pos / self.nb_iter\n",
    "        func_mom = None\n",
    "        if current_pos <= self.pct_start:\n",
    "            func_lr = self.first_step\n",
    "            func_mom = self.first_step_mom\n",
    "            relative_pos = pos / (self.nb_iter*self.pct_start)\n",
    "        else:\n",
    "            func_lr = self.last_step\n",
    "            func_mom = self.last_step_mom\n",
    "            relative_pos = (pos-(self.nb_iter*self.pct_start)) /(self.nb_iter*(1-self.pct_start))\n",
    "       \n",
    "        return func_lr(relative_pos), func_mom(relative_pos)\n",
    "    \n",
    "    \n",
    "class FlatCyle:\n",
    "    def __init__(self, lr_max, moms=(0.9, 0.9), pct_start=0.7, final_div=1e4):\n",
    "        self.lr_max,self.pct_start,self.final_div = lr_max,pct_start,final_div\n",
    "        self.mom = moms\n",
    "       \n",
    "            \n",
    "    def start_training(self, nb_iter):\n",
    "        self.first_step = sched_no(self.lr_max, self.lr_max)\n",
    "        self.last_step = sched_cos(self.lr_max, self.lr_max/self.final_div)\n",
    "        \n",
    "        self.first_step_mom = sched_no(self.mom[0], self.mom[1])\n",
    "        self.last_step_mom = sched_no(self.mom[1], self.mom[0])\n",
    "        \n",
    "        self.nb_iter = nb_iter\n",
    "        \n",
    "    def get_params(self, pos):\n",
    "        current_lr = None\n",
    "        func_lr = None\n",
    "        current_pos = pos / self.nb_iter\n",
    "        func_mom = None\n",
    "        if current_pos <= self.pct_start:\n",
    "            func_lr = self.first_step\n",
    "            func_mom = self.first_step_mom\n",
    "            relative_pos = pos / (self.nb_iter*self.pct_start)\n",
    "        else:\n",
    "            func_lr = self.last_step\n",
    "            func_mom = self.last_step_mom\n",
    "            relative_pos = (pos-(self.nb_iter*self.pct_start)) /(self.nb_iter*(1-self.pct_start))\n",
    "       \n",
    "        return func_lr(relative_pos), func_mom(relative_pos)\n",
    "\n",
    "class OneCyle:\n",
    "    def __init__(self, lr_max, moms=(0.95, 0.85), div_factor=25., pct_start=0.3, final_div=None):\n",
    "        self.lr_max,self.div_factor,self.pct_start,self.final_div = lr_max,div_factor,pct_start,final_div\n",
    "        self.mom = moms\n",
    "        if final_div is None: self.final_div = div_factor*1e4\n",
    "            \n",
    "    def start_training(self, nb_iter):\n",
    "        self.first_step = sched_cos(self.lr_max/self.div_factor, self.lr_max)\n",
    "        self.last_step = sched_cos(self.lr_max, self.lr_max/self.final_div)\n",
    "        \n",
    "        self.first_step_mom = sched_cos(self.mom[0], self.mom[1])\n",
    "        self.last_step_mom = sched_cos(self.mom[1], self.mom[0])\n",
    "        \n",
    "        self.nb_iter = nb_iter\n",
    "        \n",
    "    def get_params(self, pos):\n",
    "        current_lr = None\n",
    "        func_lr = None\n",
    "        current_pos = pos / self.nb_iter\n",
    "        func_mom = None\n",
    "        if current_pos <= self.pct_start:\n",
    "            func_lr = self.first_step\n",
    "            func_mom = self.first_step_mom\n",
    "            relative_pos = pos / (self.nb_iter*self.pct_start)\n",
    "        else:\n",
    "            func_lr = self.last_step\n",
    "            func_mom = self.last_step_mom\n",
    "            relative_pos = (pos-(self.nb_iter*self.pct_start)) /(self.nb_iter*(1-self.pct_start))\n",
    "       \n",
    "        return func_lr(relative_pos), func_mom(relative_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lr Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class LRFinder(object):\n",
    "    \"\"\"Learning rate range test.\n",
    "    The learning rate range test increases the learning rate in a pre-training run\n",
    "    between two boundaries in a linear or exponential manner. It provides valuable\n",
    "    information on how well the network can be trained over a range of learning rates\n",
    "    and what is the optimal learning rate.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): wrapped model.\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
    "            is assumed to be the lower boundary of the range test.\n",
    "        criterion (torch.nn.Module): wrapped loss function.\n",
    "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
    "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
    "            Alternatively, can be an object representing the device on which the\n",
    "            computation will take place. Default: None, uses the same device as `model`.\n",
    "        memory_cache (boolean): if this flag is set to True, `state_dict` of model and\n",
    "            optimizer will be cached in memory. Otherwise, they will be saved to files\n",
    "            under the `cache_dir`.\n",
    "        cache_dir (string): path for storing temporary files. If no path is specified,\n",
    "            system-wide temporary directory is used.\n",
    "            Notice that this parameter will be ignored if `memory_cache` is True.\n",
    "    Example:\n",
    "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    fastai/lr_find: https://github.com/fastai/fastai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion, device=None, memory_cache=True, cache_dir=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.memory_cache = memory_cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Save the original state of the model and optimizer so they can be restored if\n",
    "        # needed\n",
    "        self.model_device = next(self.model.parameters()).device\n",
    "        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
    "        self.state_cacher.store('model', self.model.state_dict())\n",
    "        self.state_cacher.store('optimizer', self.optimizer.state_dict())\n",
    "\n",
    "        # If device is None, use the same as the model\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = self.model_device\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
    "        self.model.load_state_dict(self.state_cacher.retrieve('model'))\n",
    "        self.optimizer.load_state_dict(self.state_cacher.retrieve('optimizer'))\n",
    "        self.model.to(self.model_device)\n",
    "\n",
    "    def range_test(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "    ):\n",
    "        \"\"\"Performs the learning rate range test.\n",
    "        Arguments:\n",
    "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
    "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
    "                will only use the training loss. When given a data loader, the model is\n",
    "                evaluated after each iteration on that dataset and the evaluation loss\n",
    "                is used. Note that in this mode the test takes significantly longer but\n",
    "                generally produces more precise results. Default: None.\n",
    "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
    "            num_iter (int, optional): the number of iterations over which the test\n",
    "                occurs. Default: 100.\n",
    "            step_mode (str, optional): one of the available learning rate policies,\n",
    "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
    "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
    "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
    "                exponential smoothing. Default: 0.05.\n",
    "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
    "                threshold:  diverge_th * best_loss. Default: 5.\n",
    "        \"\"\"\n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        # Create an iterator to get data batch by batch\n",
    "        iterator = iter(train_loader)\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Get a new set of inputs and labels\n",
    "            try:\n",
    "                inputs, labels = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(train_loader)\n",
    "                inputs, labels = next(iterator)\n",
    "                \n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(inputs, labels)\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_schedule.step()\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _train_batch(self, inputs, labels):\n",
    "        # Set model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "  \n",
    "\n",
    "        # Forward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    " \n",
    "\n",
    "    def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
    "        \"\"\"Plots the learning rate range test.\n",
    "        Arguments:\n",
    "            skip_start (int, optional): number of batches to trim from the start.\n",
    "                Default: 10.\n",
    "            skip_end (int, optional): number of batches to trim from the start.\n",
    "                Default: 5.\n",
    "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
    "                scale; otherwise, plotted in a linear scale. Default: True.\n",
    "        \"\"\"\n",
    "\n",
    "        if skip_start < 0:\n",
    "            raise ValueError(\"skip_start cannot be negative\")\n",
    "        if skip_end < 0:\n",
    "            raise ValueError(\"skip_end cannot be negative\")\n",
    "\n",
    "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
    "        # properly so the behaviour is the expected\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        # Plot loss as a function of the learning rate\n",
    "        plt.plot(lrs, losses)\n",
    "        if log_lr:\n",
    "            plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float, optional): the initial learning rate which is the lower\n",
    "            boundary of the test. Default: 10.\n",
    "        num_iter (int, optional): the number of iterations over which the test\n",
    "            occurs. Default: 100.\n",
    "        last_epoch (int): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float, optional): the initial learning rate which is the lower\n",
    "            boundary of the test. Default: 10.\n",
    "        num_iter (int, optional): the number of iterations over which the test\n",
    "            occurs. Default: 100.\n",
    "        last_epoch (int): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class StateCacher(object):\n",
    "    def __init__(self, in_memory, cache_dir=None):\n",
    "        self.in_memory = in_memory\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.cache_dir is None:\n",
    "            import tempfile\n",
    "            self.cache_dir = tempfile.gettempdir()\n",
    "        else:\n",
    "            if not os.path.isdir(self.cache_dir):\n",
    "                raise ValueError('Given `cache_dir` is not a valid directory.')\n",
    "\n",
    "        self.cached = {}\n",
    "\n",
    "    def store(self, key, state_dict):\n",
    "        if self.in_memory:\n",
    "            self.cached.update({key: copy.deepcopy(state_dict)})\n",
    "        else:\n",
    "            fn = os.path.join(self.cache_dir, 'state_{}_{}.pt'.format(key, id(self)))\n",
    "            self.cached.update({key: fn})\n",
    "            torch.save(state_dict, fn)\n",
    "\n",
    "    def retrieve(self, key):\n",
    "        if key not in self.cached:\n",
    "            raise KeyError('Target {} was not cached.'.format(key))\n",
    "\n",
    "        if self.in_memory:\n",
    "            return self.cached.get(key)\n",
    "        else:\n",
    "            fn = self.cached.get(key)\n",
    "            if not os.path.exists(fn):\n",
    "                raise RuntimeError('Failed to load state in {}. File does not exist anymore.'.format(fn))\n",
    "            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
    "            return state_dict\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
    "        this instance being destroyed.\"\"\"\n",
    "        if self.in_memory:\n",
    "            return\n",
    "\n",
    "        for k in self.cached:\n",
    "            if os.path.exists(self.cached[k]):\n",
    "                os.remove(self.cached[k])\n",
    "                \n",
    "                \n",
    "def lr_finder(model, train_dl, loss_fn, params_arch={}, wd=1e-6):\n",
    "    opt = AdamW(model.parameters(), lr=1e-7, weight_decay=wd)\n",
    "    lr_find = LRFinder(model, opt, loss_fn, device=\"cuda\")\n",
    "    \n",
    "    lr_find.range_test(train_dl, end_lr=100, num_iter=100)\n",
    "    lr_find.plot(skip_end=0)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

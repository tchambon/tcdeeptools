{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#From fastai library\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.\"\n",
    "    def __init__(self, sz = None):\n",
    "        \"Output will be 2*sz or 2 if sz is None\"\n",
    "        super().__init__()\n",
    "        self.output_size = sz or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.output_size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.output_size)\n",
    "\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def icnr(x, scale=2, init=nn.init.kaiming_normal_):\n",
    "#     \"ICNR init of `x`, with `scale` and `init` function.\"\n",
    "#     ni,nf,h,w = x.shape\n",
    "#     ni2 = int(ni/(scale**2))\n",
    "#     k = init(torch.zeros([ni2,nf,h,w])).transpose(0, 1)\n",
    "#     k = k.contiguous().view(ni2, nf, -1)\n",
    "#     k = k.repeat(1, 1, scale**2)\n",
    "#     k = k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
    "#     x.data.copy_(k)\n",
    "\n",
    "# class PixelShuffle_ICNR(nn.Module):\n",
    "#     \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`, `icnr` init, and `weight_norm`.\"\n",
    "#     def __init__(self, ni:int, nf:int=None, scale:int=2, blur:bool=False, norm_type=NormType.Weight, leaky:float=None):\n",
    "#         nf = ifnone(nf, ni)\n",
    "#         self.conv = conv_layer(ni, nf*(scale**2), ks=1, norm_type=norm_type, use_activ=False)\n",
    "#         icnr(self.conv[0].weight)\n",
    "#         self.shuf = nn.PixelShuffle(scale)\n",
    "#         # Blurring over (h*w) kernel\n",
    "#         # \"Super-Resolution using Convolutional Neural Networks without Any Checkerboard Artifacts\"\n",
    "#         # - https://arxiv.org/abs/1806.02658\n",
    "#         self.pad = nn.ReplicationPad2d((1,0,1,0))\n",
    "#         self.blur = nn.AvgPool2d(2, stride=1)\n",
    "#         self.do_blur = blur\n",
    "#         self.relu = relu(True, leaky=leaky)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.shuf(self.relu(self.conv(x)))\n",
    "#         return self.blur(self.pad(x)) if self.do_blur else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# The current pytorch version has a bug on circular padding.\n",
    "\n",
    "class CircularPad2d(nn.Module):\n",
    "    def __init__(self, pad_size):\n",
    "        super().__init__()\n",
    "        self.pad_size = pad_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (self.pad_size, self.pad_size, self.pad_size, self.pad_size), mode='circular')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#from fastai\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def generate_gaussian_points(N):\n",
    "\n",
    "    # N uniformly distributed numbers in [0, 1]\n",
    "\n",
    "    U = np.linspace(0.5/float(N), 1-0.5/float(N), N)\n",
    "\n",
    "    # N Gaussian points : G = CDF^{-1}(U) = sqrt(2) * erfinv(2*U - 1)\n",
    "\n",
    "    G = 1.41421356 * scipy.special.erfinv(2*U - 1)\n",
    "\n",
    "    #\n",
    "\n",
    "    return G\n",
    "\n",
    "class OTGauss(nn.Module):\n",
    "    def __init__(self, nb_dim, output_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.nb_dim = nb_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.current_iter = 0\n",
    "        \n",
    "        self.generate_dims()\n",
    "        \n",
    "    \n",
    "    def generate_dims(self):\n",
    "        self.dims = torch.rand((self.nb_dim, self.output_size)).cuda()\n",
    "        self.dims = (self.dims / torch.norm(self.dims, dim=1).unsqueeze(1))\n",
    "        \n",
    "        if not hasattr(self, 'projeted_target'): self.generate_target()\n",
    "        \n",
    "        self.projeted_target = self.dims @ self.target\n",
    "        assert(self.projeted_target.shape == (self.nb_dim, self.batch_size))\n",
    "        self.projeted_target = torch.sort(self.projeted_target, axis=-1)[0]\n",
    "        \n",
    "    def generate_target(self):\n",
    "        self.target = torch.tensor(generate_gaussian_points(self.output_size)).unsqueeze(-1).repeat((1,self.batch_size))\n",
    "        self.target = self.target.type(torch.FloatTensor).cuda()\n",
    "        \n",
    "    def forward(self, generated):\n",
    "        proj_gen = self.dims @ generated.T\n",
    "        #print(proj_gen.shape)\n",
    "        assert(proj_gen.shape == (self.nb_dim, self.batch_size))\n",
    "\n",
    "        proj_gen = torch.sort(proj_gen, axis=-1)[0]\n",
    "\n",
    "        dist = torch.mean(((proj_gen - self.projeted_target) ** 2), axis=-1)\n",
    "        #print(f'shape dist after sum {dist.shape}')\n",
    "        assert(len(dist) == self.nb_dim)\n",
    "\n",
    "        return torch.mean(dist)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((64,512,5,5))\n",
    "ad_concat_pool = AdaptiveConcatPool2d(1)\n",
    "r = ad_concat_pool(input)\n",
    "assert(r.shape == (64,1024,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = OTGauss(5000, 128, 64)\n",
    "\n",
    "rand1 = torch.randn((64,128)).cuda()\n",
    "rand2 = torch.randn((64,128)).cuda()\n",
    "\n",
    "randu1 = torch.rand((64,128)).cuda()\n",
    "randu2 = torch.rand((64,128)).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.5740, device='cuda:0') tensor(1.2106, device='cuda:0') tensor(0.4753, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "assert(ot(rand1) < 2)\n",
    "assert(ot(rand2) < 2)\n",
    "\n",
    "assert(ot(randu1) > 10)\n",
    "\n",
    "assert(ot(randu2) > 10)\n",
    "\n",
    "g = torch.tensor(generate_gaussian_points(128),  dtype=torch.float).view(1,128).cuda()\n",
    "g = g.repeat((64, 1))\n",
    "\n",
    "assert(ot(g) == 0)\n",
    "\n",
    "col_idxs = list(range(128))\n",
    "random.shuffle(col_idxs)\n",
    "g = g[:, torch.tensor(col_idxs)]\n",
    "\n",
    "assert(ot(g) < 0.8)\n",
    "\n",
    "print(ot(randu1),ot(rand1),\n",
    "      ot(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circular padding\n",
    "r = torch.randn((3,2,5,5))\n",
    "p = CircularPad2d(2)\n",
    "\n",
    "assert(p(r).shape == (3,2,9,9))\n",
    "assert(all(p(r)[1,1,1,2:-2]  == r[1,1,-1,:]))\n",
    "assert(all(p(r)[1,1,1,2:-2]  == r[1,1,-1,:]))\n",
    "assert(all(p(r)[1,1,2:-2,2] == p(r)[1,1,2:-2,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_training.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

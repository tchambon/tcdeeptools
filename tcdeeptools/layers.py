# AUTOGENERATED! DO NOT EDIT! File to edit: 03_layers.ipynb (unless otherwise specified).

__all__ = ['AdaptiveConcatPool2d', 'CircularPad2d', 'init_cnn', 'generate_gaussian_points',
           'generate_gaussian_intervals', 'OTGauss', 'OTGaussInterval']

# Cell
import torch
from torch import nn
import scipy.special
import numpy as np
import random

# Cell

#From fastai library
class AdaptiveConcatPool2d(nn.Module):
    "Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`."
    def __init__(self, sz = None):
        "Output will be 2*sz or 2 if sz is None"
        super().__init__()
        self.output_size = sz or 1
        self.ap = nn.AdaptiveAvgPool2d(self.output_size)
        self.mp = nn.AdaptiveMaxPool2d(self.output_size)

    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)

# Cell
# The current pytorch version has a bug on circular padding.

class CircularPad2d(nn.Module):
    def __init__(self, pad_size):
        super().__init__()
        self.pad_size = pad_size

    def forward(self, x):
        return torch.nn.functional.pad(x, (self.pad_size, self.pad_size, self.pad_size, self.pad_size), mode='circular')


# Cell

#from fastai
def init_cnn(m):
    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
    for l in m.children(): init_cnn(l)

# Cell


def generate_gaussian_points(N):

    # N uniformly distributed numbers in [0, 1]

    U = np.linspace(0.5/float(N), 1-0.5/float(N), N)

    # N Gaussian points : G = CDF^{-1}(U) = sqrt(2) * erfinv(2*U - 1)

    G = 1.41421356 * scipy.special.erfinv(2*U - 1)

    #

    return G


def generate_gaussian_intervals(N):

    # N uniformly distributed numbers in [0, 1]

    U = np.linspace(0, 1, N+1)

    # N Gaussian points : G = CDF^{-1}(U) = sqrt(2) * erfinv(2*U - 1)

    G = 1.41421356 * scipy.special.erfinv(2*U - 1)

    #

    return G

class OTGauss(nn.Module):
    def __init__(self, nb_dim, output_size, batch_size, freq_reproj=0, loss_fn=None):
        super().__init__()
        self.nb_dim = nb_dim
        self.batch_size = batch_size
        self.output_size = output_size
        self.current_iter = 0
        self.freq_reproj = freq_reproj
        self.loss_fn = loss_fn

        self.generate_dims()


    def generate_dims(self):
        self.dims = torch.rand((self.nb_dim, self.output_size)).cuda()
        self.dims = (self.dims / torch.norm(self.dims, dim=1).unsqueeze(1))

        if not hasattr(self, 'projeted_target'): self.generate_target()

        self.projeted_target = self.dims @ self.target
        assert(self.projeted_target.shape == (self.nb_dim, self.batch_size))
        self.projeted_target = torch.sort(self.projeted_target, axis=-1)[0]

    def generate_target(self):
        self.target = torch.tensor(generate_gaussian_points(self.output_size)).unsqueeze(-1).repeat((1,self.batch_size))
        self.target = self.target.type(torch.FloatTensor).cuda()

    def forward(self, generated):
        if self.freq_reproj > 0:
            self.current_iter += 1
            if self.current_iter % self.freq_reproj == 0:
                self.generate_dims()
                self.current_iter = 0
        proj_gen = self.dims @ generated.T
        #print(proj_gen.shape)
        assert(proj_gen.shape == (self.nb_dim, self.batch_size))

        proj_gen = torch.sort(proj_gen, axis=-1)[0]
        if self.loss_fn is None:
            dist = torch.mean(((proj_gen - self.projeted_target) ** 2), axis=-1)
        else:
            dist = self.loss_fn(proj_gen, self.projeted_target, reduction='none')
        #print(f'shape dist after sum {dist.shape}')
        assert(len(dist) == self.nb_dim)

        return torch.mean(dist)



class OTGaussInterval(nn.Module):
    def __init__(self, nb_dim, output_size, batch_size):
        super().__init__()
        self.nb_dim = nb_dim
        self.batch_size = batch_size
        self.output_size = output_size
        self.current_iter = 0

        self.generate_dims()


    def generate_dims(self):
        self.dims = torch.rand((self.nb_dim, self.output_size)).cuda()
        self.dims = (self.dims / torch.norm(self.dims, dim=1).unsqueeze(1))

        if not hasattr(self, 'projeted_target'): self.generate_target()

        self.projeted_target = self.dims @ self.target
        assert(self.projeted_target.shape == (self.nb_dim, self.batch_size))
        self.projeted_target = torch.sort(self.projeted_target, axis=-1)[0]

    def generate_target(self):
        self.target = torch.tensor(generate_gaussian_points(self.output_size)).unsqueeze(-1).repeat((1,self.batch_size))
        self.target = self.target.type(torch.FloatTensor).cuda()

    def forward(self, generated):
        proj_gen = self.dims @ generated.T
        #print(proj_gen.shape)
        assert(proj_gen.shape == (self.nb_dim, self.batch_size))

        proj_gen = torch.sort(proj_gen, axis=-1)[0]



        dist = torch.mean(((proj_gen - self.projeted_target) ** 2), axis=-1)
        #print(f'shape dist after sum {dist.shape}')
        assert(len(dist) == self.nb_dim)

        return torch.mean(dist)